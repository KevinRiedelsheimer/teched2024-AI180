# Exercise 2 - Prompt Templating

In this exercise, you will learn how to send prompts with client-side prompt templates.

### 1. Navigate to the Function 
Open [orchestration.ts](../app/src/orchestration.ts) file and search for the function `orchestrationCompletionTemplate`.

### 2. Add Implementation
Type or uncomment the following code in the function `orchestrationCompletionTemplate`:

```typescript
const orchestrationClient = new OrchestrationClient({
    llm: {
        model_name: 'meta--llama3-70b-instruct',
        model_params: { max_tokens: 1000, temperature: 0.1 }
    },
    templating: {
        template: [
            { role: 'system', content: 'Please generate contents with HTML tags.' },
            {
                role: 'user',
                content: 'Create a job post for the position: {{?position}}.'
            }
        ]
    }
});
const response = await orchestrationClient.chatCompletion({
    inputParams: { position: 'Java dev' }
});
return response.getContent();
```

> [!NOTE]
> In addition to the code structure in the [previous exercise](../ex1/README.md#2-add-implementation), you might notice the following changes:
> - An additional model option (`temperature`) is configured.
> - A system prompt is added.
> - A client-side prompt template is used with a placeholder (`position`).
> - An input parameter is passed, for providing the value of the variable `position`, when calling the chat completion endpoint.

### 3. Restart the Application
Save your changes and wait for the application to automatically restart.

### 4. Check the LLM Response
Open your browser and visit http://localhost:8080/orchestration/template. 
You should see a nice HTML page generated by the LLM.

## Summary TODO

You've now ...

Continue to - [Exercise 3 - Excercise 3 ](../ex3/README.md)
